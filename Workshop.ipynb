{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eOB8tffT11c"
      },
      "source": [
        "# Atmospheric Data Handling Workshop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgOFzPTtjTBH"
      },
      "source": [
        "## 1. Extracting MODIS MCD19A2CMG product data and storing it as dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6kqPc4wjaYf"
      },
      "source": [
        "Clone GitHub Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvQTKBWJzcxJ",
        "outputId": "488fe1c9-6b43-45b3-f366-f94ebcdc44ba"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/msramadan/Atmospheric-Data-Handling-Workshop.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmqv0oQLjg-f"
      },
      "source": [
        "Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBwiQmGSy55O",
        "outputId": "59ea6af9-247d-41bf-9bd6-f0bea39783cd"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -r https://raw.githubusercontent.com/msramadan/Atmospheric-Data-Handling-Workshop/refs/heads/main/HDF_nc_requirements.txt -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNffhzyojykW"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AcjFiFSy5Kj"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import rasterio\n",
        "from pyhdf.SD import SD, SDC\n",
        "import geopandas as gpd\n",
        "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
        "import cartopy.crs as ccrs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwxBcvYQqfOR"
      },
      "source": [
        "Here we define a function that extract the study area sub-array from a single HDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWVMo263y5Kr"
      },
      "outputs": [],
      "source": [
        "def hdf_to_2D_array(hdf, lon_bounds, lat_bounds, var_name, cloud_frac_name, cloud_mask):\n",
        "    \"\"\"\n",
        "    hdf: a hdf file read using pyhdf.SD\n",
        "    lon_bounds: a list containing two values; longitude extracting bounds (small to big).\n",
        "    lat_bounds: a list containing two values; latitude extracting bounds (big to small).\n",
        "    var_name: a string represents the name of the variable in the dataset that needs to be extracted.\n",
        "    cloud_frac_name: a string represents the name of cloud fraction varianble in the dataset.\n",
        "    cloud_mask: a float of the cloud_mask percentage (eg. 0.01 keeps the values of the pixels with cloud cover percentage lower than 0.01)\n",
        "    Returns a 2-D array of the extracted data.\n",
        "    \"\"\"\n",
        "\n",
        "    cloud = hdf.select(cloud_frac_name)[:]\n",
        "    aod = hdf.select(var_name)[:]\n",
        "\n",
        "\n",
        "\n",
        "    aod_s = aod[int((90-lat_bounds[0])/0.05):int((90-lat_bounds[1])/0.05),int((lon_bounds[0]+180)/.05):int((lon_bounds[1]+180)/.05)]\n",
        "    aod_s = np.where(aod_s == hdf.select(var_name).attributes()['_FillValue'], np.nan, aod_s)\n",
        "    aod_s = aod_s * hdf.select(var_name).attributes()['scale_factor']\n",
        "    cloud_s = cloud[int((90-lat_bounds[0])/0.05):int((90-lat_bounds[1])/0.05),int((lon_bounds[0]+180)/.05):int((lon_bounds[1]+180)/.05)]\n",
        "    cloud_s = np.where(cloud_s == hdf.select(cloud_frac_name).attributes()['_FillValue'], np.nan, cloud_s)\n",
        "    cloud_s = cloud_s * hdf.select(cloud_frac_name).attributes()['scale_factor']\n",
        "\n",
        "    cloud_s[cloud_s > cloud_mask] = np.nan\n",
        "    cloud_s[cloud_s <= cloud_mask] = 1\n",
        "\n",
        "    aod_new = np.multiply(aod_s, cloud_s)\n",
        "\n",
        "    return aod_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X79AECWyry2I"
      },
      "source": [
        "Define the folder directory containing HDF files and files names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAJGkf1uy5Kr"
      },
      "outputs": [],
      "source": [
        "folder = \"Atmospheric-Data-Handling-Workshop/Workshop Data/MODIS MCD19A2CMG/\"\n",
        "files = [file for file in os.listdir(folder) if file.endswith('.hdf')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IK73XEC4y5Kr",
        "outputId": "f06ed3f3-e1b9-4320-c329-28e4cd90d5e1"
      },
      "outputs": [],
      "source": [
        "files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2siN9-UrsF7w"
      },
      "source": [
        "Now we iterate over all HDF files, extract the a 2-D array of each file, store them in 3-D array, give this array longitude, latitude, and time indices and make it a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no_6DYm3y5Ks"
      },
      "outputs": [],
      "source": [
        "# Define linear spaces for the HDF dimensions\n",
        "lon = np.arange(-180,180,.05)\n",
        "lat = np.arange(90,-90,-.05)\n",
        "\n",
        "# Define lon and lat bounds to be extracted\n",
        "lons = [34.75, 39.75]\n",
        "lats = [33.75, 28.75]\n",
        "\n",
        "# Define Date-Time index that represents the HDF files' Dates\n",
        "dates = pd.DataFrame(columns=['Name', 'Year', 'Day', 'Date'])\n",
        "dates['Name'] = files\n",
        "for i in range(len(dates)):\n",
        "    dates.loc[i, 'Year'] = dates['Name'][i][12:16]\n",
        "    dates.loc[i, 'Day'] = dates['Name'][i][16:19]\n",
        "dts = pd.to_datetime(dates['Year'].astype(str) + dates['Day'].astype(str), format='%Y%j')\n",
        "dates['Date'] = pd.DatetimeIndex(dts)\n",
        "dates.sort_values(by='Date', inplace=True)\n",
        "dates.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Define NetCDF indices\n",
        "nc_dates = dates['Date']\n",
        "nc_lons = lon[int((lons[0]+180)/.05):int((lons[1]+180)/.05)]\n",
        "nc_lats = lat[int((90-lats[0])/0.05):int((90-lats[1])/0.05)]\n",
        "\n",
        "# Define 3-d array of NaN\n",
        "nc_year = np.zeros((len(nc_dates), len(nc_lats), len(nc_lons))) * np.nan\n",
        "\n",
        "# Read each file and store its values in the corresponding day in the 3-d array\n",
        "for i in range(len(dates)):\n",
        "    ds = SD(folder + dates['Name'][i], SDC.READ)\n",
        "    nc_year[i] = hdf_to_2D_array(ds, lons, lats, 'AOD_055', 'CloudFraction', 0.01)\n",
        "\n",
        "# Create NetCDF for the extracted Data\n",
        "data_set = xr.Dataset(\n",
        "    {\n",
        "        'AOD_055': ((\"time\", \"latitude\", \"longitude\"), nc_year)\n",
        "    },\n",
        "    coords={\n",
        "        \"time\": nc_dates,\n",
        "        \"latitude\": nc_lats,\n",
        "        \"longitude\": nc_lons\n",
        "\n",
        "\n",
        "    }\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "5g7W_7P5BjiD",
        "outputId": "c9c4f6dd-ccdb-4ffb-fb62-38f345c2a1a6"
      },
      "outputs": [],
      "source": [
        "data_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gerju7lzssMf"
      },
      "source": [
        "Import Jordan's borders shapefile and define its projection (WGS84)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "tOYc9XRdK1Ea",
        "outputId": "aa7c2190-1ff1-4dee-bd97-72138a903de3"
      },
      "outputs": [],
      "source": [
        "jor = gpd.read_file(\"Atmospheric-Data-Handling-Workshop/Jordan_Borders_shp/\")\n",
        "jor.set_crs('epsg:4326', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUonGw2HtH-w"
      },
      "source": [
        "Visualize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "2307c449",
        "outputId": "8b2efd80-048d-494d-f30c-65f5f35ee848"
      },
      "outputs": [],
      "source": [
        "# Select the 'AOD_055' variable from the dataset\n",
        "aod_variable = data_set['AOD_055']\n",
        "\n",
        "# Create a plot for each time step (day)\n",
        "# Use fig and axes to plot the border on each subplot\n",
        "g = aod_variable.plot(col='time', col_wrap=5, figsize=(18, 8))\n",
        "\n",
        "# Add the Jordan border to each subplot\n",
        "for ax in g.axes.flat:\n",
        "    jor.plot(ax=ax, facecolor='none', edgecolor='r')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffbc23e4"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/Atmospheric-Data-Handling-Workshop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79dAHT-PIEzx"
      },
      "source": [
        "## 2. Spatial resampling and data assimilation techniques for harmonizing multi-source datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbg1UpOwZ5u0"
      },
      "source": [
        "Clone GitHub Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcwqxQgtDO_1",
        "outputId": "7eba476b-ce2a-4833-fd99-dbcca58bfbc9"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/msramadan/Rest-of-Workshop.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQB59AfQZ6U0"
      },
      "source": [
        "Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edbzJL2EWilp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "from shapely.geometry import Polygon\n",
        "from shapely.geometry import box\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxmR1zlFaAXq"
      },
      "source": [
        "import datasets have different spatial and temporal resolutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14REoUEpG3hn"
      },
      "outputs": [],
      "source": [
        "t2m = xr.open_dataset(\n",
        "    'Rest-of-Workshop/Data/NetCDF4/t2m_workshop.nc'\n",
        "    ).drop(['number', 'expver']).sel(valid_time=slice(\n",
        "        '2025-06-22', '2025-06-28'\n",
        "        ))\n",
        "data_set = xr.open_dataset(\n",
        "    'Rest-of-Workshop/Data/Extracted_nc/Extracted_AOD.nc'\n",
        "    ).sel(time=slice(\n",
        "        '2025-06-22', '2025-06-28'\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6XHq2WlaQW8"
      },
      "source": [
        "The temperature dataset has hourly data with 0.25 x 0.25 grids\n",
        "\n",
        "While AOD dataset is daily with 0.05 x 0.05 grids\n",
        "\n",
        "we need them to have matching resolutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "BAuQbcZaICMh",
        "outputId": "c8488d1a-3941-4674-dae1-d745f1f30d53"
      },
      "outputs": [],
      "source": [
        "t2m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbGBzBPVapLj"
      },
      "source": [
        "Convert the temperature dataset into daily dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "65hAL5yzH6nL",
        "outputId": "7c0ab589-ba2f-42d1-8e0d-fe32b60c50eb"
      },
      "outputs": [],
      "source": [
        "t2m = t2m.coarsen(valid_time=24).mean()\n",
        "t2m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "JBGGKFCvIsal",
        "outputId": "9d1574e7-f128-45a3-96e3-afffe7c1e728"
      },
      "outputs": [],
      "source": [
        "t2m['valid_time'] = pd.DatetimeIndex(t2m['valid_time'].dt.date)\n",
        "t2m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTgDR78na4qS"
      },
      "source": [
        "Define the spatial resampling function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-SC11sSKHt-"
      },
      "outputs": [],
      "source": [
        "def fine_grid_no_interpolation(cdf, regridding_factor, epsilon = 1e-9):\n",
        "\n",
        "    \"\"\"\n",
        "    cdf: NetCDF4 dataset read by xarray.\n",
        "\n",
        "    regridding_factor: the factor that we want to resample our dataset by; it has to be a multiplier of the original dimension,\n",
        "    Eg. using a factor of 5 for 0.25 ERA5 will resample the data into 0.05 degree dataset.\n",
        "\n",
        "    epsilon: a value to avoid floating point error when defining the new dimensions.\n",
        "\n",
        "    This function resamples the dataset into finer dimension without interpolation, using Kroniker Product.\n",
        "\n",
        "    Returns xarray dataset with the new spatial dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    lons = cdf.variables[[name for name in cdf.coords if 'lon' in name.lower()][0]][:]\n",
        "    lats = cdf.variables[[name for name in cdf.coords if 'lat' in name.lower()][0]][:]\n",
        "    var = cdf.variables[list(cdf.data_vars.keys())[0]][:]\n",
        "    dif_lon = lons[1] - lons[0]\n",
        "    dif_lat = lats[0] - lats[1]\n",
        "\n",
        "    lats_fine = np.float64(np.round(np.arange(lats[0] + (dif_lat/2) - (dif_lat/regridding_factor)/2,\n",
        "                               lats[-1]- (dif_lat/2) + (dif_lat/regridding_factor)/2,\n",
        "                               -dif_lat/regridding_factor+epsilon),4))\n",
        "\n",
        "    lons_fine = np.float64(np.round(np.arange(lons[0] - (dif_lon/2) + (dif_lon/regridding_factor)/2,\n",
        "                               lons[-1]+ (dif_lon/2) - (dif_lon/regridding_factor)/2,\n",
        "                               dif_lon/regridding_factor-epsilon),4))\n",
        "\n",
        "    t = cdf[[name for name in cdf.coords if 'time' in name.lower()][0]]\n",
        "    times = pd.DatetimeIndex(t.to_pandas())\n",
        "\n",
        "    var_kron = np.kron(var, np.ones((regridding_factor,regridding_factor)))\n",
        "\n",
        "    var_name = list(cdf.data_vars.keys())[0]\n",
        "\n",
        "    # Create a DataArray for the variable and then convert to a Dataset\n",
        "    data_array = xr.DataArray(\n",
        "        var_kron,\n",
        "        coords=[times, lats_fine, lons_fine],\n",
        "        dims=[\"time\", \"latitude\", \"longitude\"],\n",
        "        name=var_name\n",
        "    )\n",
        "    data_set = data_array.to_dataset()\n",
        "\n",
        "\n",
        "    return data_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0B2AxKjTzto"
      },
      "outputs": [],
      "source": [
        "t2m_res = fine_grid_no_interpolation(t2m, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "gSK7FJoaWKE2",
        "outputId": "65e406ef-9dac-4d48-b11b-0b841437bee4"
      },
      "outputs": [],
      "source": [
        "t2m_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "OZMBlPJwshsr",
        "outputId": "ea6c0a13-86f1-42e3-8643-40416a911722"
      },
      "outputs": [],
      "source": [
        "Image(url=\"https://raw.githubusercontent.com/msramadan/Rest-of-Workshop/main/kron1.png\", width=800, height=400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI8n55zHbSTN"
      },
      "source": [
        "Round the spatial dimension to avoid floating point errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh4lRjqBXxdl"
      },
      "outputs": [],
      "source": [
        "def round_coords(ds, decimal=6):\n",
        "    for coord in ['latitude', 'lat']:\n",
        "        if coord in ds.coords:\n",
        "            ds = ds.assign_coords({coord: np.round(ds[coord].values, decimal)})\n",
        "    for coord in ['longitude', 'lon']:\n",
        "        if coord in ds.coords:\n",
        "            ds = ds.assign_coords({coord: np.round(ds[coord].values, decimal)})\n",
        "    return ds\n",
        "\n",
        "decimal_places = 6\n",
        "data_set = round_coords(data_set, decimal_places)\n",
        "t2m_res = round_coords(t2m_res, decimal_places)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5axYIB5baXK"
      },
      "source": [
        "Merged the two datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "9XzktCG4Vmgi",
        "outputId": "1cc49354-8b03-451c-d0a6-b1e23c8f5eb7"
      },
      "outputs": [],
      "source": [
        "merged = xr.merge([data_set, t2m_res])\n",
        "merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "_VGwXGT-YAoi",
        "outputId": "8cf12f55-06e8-489f-8ab8-212a7fc81a87"
      },
      "outputs": [],
      "source": [
        "merged.to_dataframe().dropna(subset='t2m').reset_index().sort_values(by=['latitude'], ascending=False).head(20).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djp3-MAzvauI"
      },
      "source": [
        "## 3. Edge-Weighted Averaging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2SMUEsZv8RR"
      },
      "source": [
        "A custom method for estimating time series averages for small polygons situated between grid cells\n",
        "\n",
        "Define the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwApeK6nuAcS"
      },
      "outputs": [],
      "source": [
        "def extracting_and_averaging_polygon(poly, nc_file, regridding_factor, epsilon = 1e-9):\n",
        "  poly_4326 = poly.to_crs('epsg:4326')\n",
        "  x = poly_4326.centroid.x.values[0]\n",
        "  y = poly_4326.centroid.y.values[0]\n",
        "  lons = nc_file.variables[[name for name in nc_file.coords if 'lon' in name.lower()][0]][:]\n",
        "  lats = nc_file.variables[[name for name in nc_file.coords if 'lat' in name.lower()][0]][:]\n",
        "  dif_lon = lons[1] - lons[0]\n",
        "  dif_lat = lats[0] - lats[1]\n",
        "\n",
        "  margin_lon = dif_lon/2 * 1.5\n",
        "  margin_lat = dif_lat/2 * 1.5\n",
        "\n",
        "  bounds = poly_4326['geometry'][0].bounds  # Get bounds in EPSG:4326\n",
        "  maxi = max(max(abs(x - (bounds[0] - margin_lon)), abs(x - (bounds[2] + margin_lon))),\n",
        "           max(abs(y - (bounds[1] - margin_lat)), abs(y - (bounds[3] + margin_lat))))\n",
        "\n",
        "  rectangle_polygon_geometry = box(x - maxi - margin_lon,\n",
        "                                 y - maxi + margin_lat,\n",
        "                                 x + maxi + margin_lon,\n",
        "                                 y + maxi - margin_lat)\n",
        "\n",
        "  # Create the GeoDataFrame\n",
        "  polygon = gpd.GeoDataFrame(index=[0], crs='epsg:4326', geometry=[rectangle_polygon_geometry])\n",
        "  nc_file.rio.set_spatial_dims(x_dim=\"longitude\", y_dim=\"latitude\", inplace=True)\n",
        "  nc_file.rio.write_crs(\"epsg:4326\", inplace=True)\n",
        "  clipped = nc_file.rio.clip(polygon.geometry,polygon.crs)\n",
        "  del lons, lats\n",
        "\n",
        "  # Rename dimensions to match expected names in fine_grid_no_interpolation\n",
        "  # Assuming the time dimension is named 'valid_time' in the input nc_file (t2m)\n",
        "  original_time_dim = [name for name in clipped.dims if 'time' in name.lower()][0]\n",
        "  clipped = clipped.rename({original_time_dim: 'time'})\n",
        "\n",
        "\n",
        "  data_set = fine_grid_no_interpolation(clipped, regridding_factor, epsilon)\n",
        "  data_set.rio.set_spatial_dims(x_dim=[name for name in data_set.coords if 'lon' in name.lower()][0],\n",
        "                                y_dim=[name for name in data_set.coords if 'lat' in name.lower()][0],\n",
        "                                inplace=True)\n",
        "  data_set.rio.write_crs(\"epsg:4326\", inplace=True)\n",
        "\n",
        "  c = data_set.rio.clip(poly.geometry,polygon.crs)\n",
        "\n",
        "  # Convert to DataFrame and inspect before groupby\n",
        "  df_before_groupby = c.to_dataframe().dropna().reset_index()\n",
        "\n",
        "\n",
        "  avg_df = df_before_groupby.groupby('time')[list(c.data_vars.keys())[0]].mean().reset_index()\n",
        "  return avg_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5j05ynYC0Xz"
      },
      "source": [
        "Importing a polygon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "Hb1433ow26Zr",
        "outputId": "929c851a-eac6-4405-8780-be54b5168047"
      },
      "outputs": [],
      "source": [
        "gdf = gpd.read_file('Rest-of-Workshop/polygons/Irbid.shp')\n",
        "gdf.set_crs('epsg:4326', inplace=True) # Set the current CRS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzAPBM8n59Hz"
      },
      "source": [
        "Lets see what the function returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "IPCX0IJLC9Gv",
        "outputId": "80a38054-2187-4a1e-9dfb-2dc544b210d1"
      },
      "outputs": [],
      "source": [
        "extracting_and_averaging_polygon(gdf, t2m, 25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2Uo_yJEADX5"
      },
      "outputs": [],
      "source": [
        "raw_ds = t2m\n",
        "fine_ds = fine_grid_no_interpolation(t2m, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "J_vSvwCqAizE",
        "outputId": "e923fb06-0863-4576-bf3c-3c96d5ffc104"
      },
      "outputs": [],
      "source": [
        "up = 32.75\n",
        "down = 32\n",
        "left = 35.5\n",
        "right = 36.25\n",
        "rectangle_polygon_geometry = box(left - .125,\n",
        "                                 down - .125,\n",
        "                                 right + .125,\n",
        "                                 up + .125)\n",
        "\n",
        "  # Create the GeoDataFrame\n",
        "polygon = gpd.GeoDataFrame(index=[0], crs='epsg:4326', geometry=[rectangle_polygon_geometry])\n",
        "\n",
        "raw_ds.rio.set_spatial_dims(x_dim=\"longitude\", y_dim=\"latitude\", inplace=True)\n",
        "fine_ds.rio.set_spatial_dims(x_dim=\"longitude\", y_dim=\"latitude\", inplace=True)\n",
        "\n",
        "raw_ds.rio.write_crs(\"epsg:4326\", inplace=True)\n",
        "fine_ds.rio.write_crs(\"epsg:4326\", inplace=True)\n",
        "\n",
        "raw_ds1 = raw_ds.rio.clip(polygon.geometry,polygon.crs)\n",
        "fine_ds1 = fine_ds.rio.clip(gdf.geometry,polygon.crs)\n",
        "\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, subplot_kw={'projection': ccrs.PlateCarree()}, sharex=True, sharey=True, figsize=(28,10), dpi=100)\n",
        "timestep = 4\n",
        "data = raw_ds1[\"t2m\"].isel(valid_time=timestep) - 273.15  # Adjust variable name accordingly\n",
        "                                                #Here we converted temperature from Kelvin into Celecius\n",
        "                                                #You can delete the '-273.15' if you are working on different variable\n",
        "\n",
        "# Extract coordinates\n",
        "lon = raw_ds1[\"longitude\"]  # Adjust if named differently\n",
        "lat = raw_ds1[\"latitude\"]\n",
        "\n",
        "mesh = ax[0].pcolormesh(lon, lat, data, cmap=\"RdBu_r\", shading=\"auto\", edgecolors=\"white\", linewidth=.5)\n",
        "ax[0].scatter(np.meshgrid(lon, lat)[0], np.meshgrid(lon, lat)[1], color='black', linewidth=0.5)\n",
        "#cbar = plt.colorbar(mesh, ax=ax[0], orientation=\"vertical\")\n",
        "#cbar.ax.tick_params(labelsize=17)\n",
        "ax[0].set_title('Original', size=26)\n",
        "\n",
        "gridliner = ax[0].gridlines(draw_labels=True, linestyle=\"None\", linewidth=0.0)\n",
        "gridliner.xformatter = LONGITUDE_FORMATTER\n",
        "gridliner.yformatter = LATITUDE_FORMATTER\n",
        "\n",
        "\n",
        "#gridliner.xlocator = mticker.FixedLocator(range(-180, 181, 1))  # Adjust longitude step\n",
        "#gridliner.ylocator = mticker.FixedLocator(range(-90, 91, 1))    # Adjust latitude step\n",
        "\n",
        "gridliner.right_labels = False  # Remove right labels\n",
        "gridliner.top_labels = False    # Remove top labels\n",
        "gridliner.xlabel_style = {'size': 16, 'rotation':45}\n",
        "gridliner.ylabel_style = {'size': 16}\n",
        "\n",
        "data = fine_ds1[\"t2m\"].isel(time=timestep)- 273.15\n",
        "\n",
        "lon = fine_ds1[\"longitude\"]  # Adjust if named differently\n",
        "lat = fine_ds1[\"latitude\"]\n",
        "\n",
        "mesh = ax[1].pcolormesh(lon, lat, data, cmap=\"RdBu_r\", shading=\"auto\", edgecolors=\"white\", linewidth=.5) #try edgecolors=\"None\"\n",
        "ax[1].set_title('Resampled Clip', size=26)\n",
        "\n",
        "gridliner = ax[1].gridlines(draw_labels=True, linestyle=\"None\", linewidth=0.0)\n",
        "gridliner.xformatter = LONGITUDE_FORMATTER\n",
        "gridliner.yformatter = LATITUDE_FORMATTER\n",
        "    #gridliner.xlocator = mticker.FixedLocator(range(-180, 181, 10))  # Adjust longitude step\n",
        "    #gridliner.ylocator = mticker.FixedLocator(range(-90, 91, 10))    # Adjust latitude step\n",
        "gridliner.right_labels = False  # Remove right labels\n",
        "gridliner.top_labels = False    # Remove top labels\n",
        "gridliner.xlabel_style = {'size': 16, 'rotation':45}\n",
        "gridliner.ylabel_style = {'size': 16}\n",
        "\n",
        "#cbar = plt.colorbar(mesh, ax=ax[1], orientation=\"vertical\")\n",
        "#cbar.ax.tick_params(labelsize=17)\n",
        "gdf.plot(ax=ax[0], edgecolor='k', facecolor='none', linewidth=3)\n",
        "gdf.plot(ax=ax[1], edgecolor='k', facecolor='none', linewidth=.5)\n",
        "plt.suptitle(f'Temperature at: {(raw_ds[\"valid_time\"][timestep-1]).dt.date.values}', size=32)\n",
        "cbar = fig.colorbar(mesh, ax=ax, location='right', shrink=1, aspect=15)\n",
        "cbar.ax.tick_params(labelsize=15)\n",
        "cbar.set_label('\\n$^o$C', size=22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYiSp9-OBxcR"
      },
      "source": [
        "## Creating spatially and temporally lagged features for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOV3zqGt_XvW"
      },
      "outputs": [],
      "source": [
        "ds = xr.open_dataset('Rest-of-Workshop/Data/NetCDF4/t2m_workshop.nc').drop(['number', 'expver'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "PZU2KD1eJk_b",
        "outputId": "bbc329ab-522c-4520-ef60-00aadcebe182"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk1esp8PIxCb"
      },
      "outputs": [],
      "source": [
        "x = np.array([[[ 0,  1,  2,  3],\n",
        "               [ 4,  5,  6,  7],   #Day 1\n",
        "               [ 8,  9, 10, 11],\n",
        "               [12, 13, 14, 15]],\n",
        "\n",
        "              [[16, 17, 18, 19],\n",
        "               [20, 21, 22, 23],   #Day 2\n",
        "               [24, 25, 26, 27],\n",
        "               [28, 29, 30, 31]],\n",
        "\n",
        "              [[32, 33, 34, 35],\n",
        "               [36, 37, 38, 39],   #Day 3\n",
        "               [40, 41, 42, 43],\n",
        "               [44, 45, 46, 47]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnipIdApJO2y",
        "outputId": "ca4b9661-ca83-4894-c88d-bd15a834a958"
      },
      "outputs": [],
      "source": [
        "x.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MIYT6wTAaGdE",
        "outputId": "489fdf07-9e1c-42b9-cb2c-f1116dfef211"
      },
      "outputs": [],
      "source": [
        "s_df = pd.DataFrame(columns=['values'], data=x.flatten())\n",
        "s_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExiJFafadWyf"
      },
      "source": [
        "Temporal lags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dS-79gjKag-1",
        "outputId": "54cf6c53-e5b4-4f19-db72-8f4abb568cf0"
      },
      "outputs": [],
      "source": [
        "s_df['values_t-1'] = s_df['values'].shift(len(x[0].flatten()))\n",
        "s_df['values_t+2'] = s_df['values'].shift(-len(x[0].flatten()))\n",
        "s_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "vjNXrwJxcQPl",
        "outputId": "96c105ae-92b6-4daf-81a5-1b97c4e96e0f"
      },
      "outputs": [],
      "source": [
        "s_df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9LKEPWUcU1t"
      },
      "source": [
        "Spatial lag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDe4QS4OfN0a"
      },
      "outputs": [],
      "source": [
        "col_name = 'values'\n",
        "len_lons = 4\n",
        "i = 1\n",
        "in_df = pd.DataFrame(columns=['values'], data=x.flatten())\n",
        "in_df[col_name + '_' + f'u{i}'] = in_df[col_name].shift(len_lons * (i))\n",
        "in_df[col_name + '_' + f'lw{i}'] = in_df[col_name].shift(-len_lons * (i))\n",
        "in_df[col_name + '_' + f'le{i}'] = in_df[col_name].shift((i))\n",
        "in_df[col_name + '_' + f'r{i}'] = in_df[col_name].shift(-(i))\n",
        "in_df[col_name + '_' + f'u-le{i}'] = in_df[col_name].shift(len_lons * (i) + (i))\n",
        "in_df[col_name + '_' + f'lw-r{i}'] = in_df[col_name].shift(-len_lons * (i) - (i))\n",
        "in_df[col_name + '_' + f'u-r{i}'] = in_df[col_name].shift(len_lons * (i) - (i))\n",
        "in_df[col_name + '_' + f'lw-le{i}'] = in_df[col_name].shift(-len_lons * (i) + (i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MHV-DU0qgDKI",
        "outputId": "6f6f415f-3d40-4a4b-8424-1ec217f5ca2f"
      },
      "outputs": [],
      "source": [
        "in_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PGPxp58GgZVK",
        "outputId": "0481eabc-88e9-4b68-ec09-f3c18144cfcb"
      },
      "outputs": [],
      "source": [
        "print(x)\n",
        "in_df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Nhe3cGqkg-"
      },
      "source": [
        "Define time lag generating function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz3eJ-UzqVw3"
      },
      "outputs": [],
      "source": [
        "def time_lag_features(in_df, col_name, lag_steps):\n",
        "  \"\"\"\n",
        "    in_df: input df that has the feature that we want to create temporally lagged feature for\n",
        "    col_name: a string representing the column name of the feature\n",
        "    lag_steps: how many lagged steps you want to generate (in both directions)\n",
        "  \"\"\"\n",
        "  lag = len(in_df[in_df['valid_time'] == in_df['valid_time'].unique()[0]])\n",
        "  for i in range(lag_steps):\n",
        "    in_df[col_name + '_' + f'lag{i+1}'] = in_df[col_name].shift(lag * (i+1))\n",
        "    in_df[col_name + '_' + f'lag-{i+1}'] = in_df[col_name].shift(lag * -(i+1)) # hash this line if you dont want the procceeding time teps\n",
        "  return in_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm1YqWy2rke7"
      },
      "source": [
        "Define spatial lag features generating function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6nGkd9DrpEt"
      },
      "outputs": [],
      "source": [
        "def spatial_lag_features(in_df, col_name, shift_list):\n",
        "    \"\"\"\n",
        "    in_df: dataframe that contains the target column.\n",
        "    col_name: name of the target column.\n",
        "    shift_list: list of shift values to generate.\n",
        "    \"\"\"\n",
        "\n",
        "    len_lons = len(in_df['longitude'].unique())\n",
        "    for i in shift_list:\n",
        "        in_df[col_name + '_' + f'u{i}'] = in_df[col_name].shift(len_lons * (i))\n",
        "        in_df[col_name + '_' + f'lw{i}'] = in_df[col_name].shift(-len_lons * (i))\n",
        "        in_df[col_name + '_' + f'le{i}'] = in_df[col_name].shift((i))\n",
        "        in_df[col_name + '_' + f'r{i}'] = in_df[col_name].shift(-(i))\n",
        "        in_df[col_name + '_' + f'u-le{i}'] = in_df[col_name].shift(len_lons * (i) + (i))\n",
        "        in_df[col_name + '_' + f'lw-r{i}'] = in_df[col_name].shift(-len_lons * (i) - (i))\n",
        "        in_df[col_name + '_' + f'u-r{i}'] = in_df[col_name].shift(len_lons * (i) - (i))\n",
        "        in_df[col_name + '_' + f'lw-le{i}'] = in_df[col_name].shift(-len_lons * (i) + (i))\n",
        "    return in_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "kuH9i7yMIMz-",
        "outputId": "c151aece-b81e-4c39-e4ac-8fafa54a10d1"
      },
      "outputs": [],
      "source": [
        "df = ds.to_dataframe().reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpGJpm3yTWaC"
      },
      "outputs": [],
      "source": [
        "df.sort_values(['valid_time', 'latitude', 'longitude'], ascending=[True, False, True], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy48uTssS_J6",
        "outputId": "c1229dfd-22a5-4178-d317-eb8355443ecf"
      },
      "outputs": [],
      "source": [
        "np.unique(ds['t2m'].values.flatten() == df['t2m'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lopo-iSNH78U"
      },
      "outputs": [],
      "source": [
        "prac = ds.to_dataframe().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vTrbQkYslWe"
      },
      "outputs": [],
      "source": [
        "prac = time_lag_features(prac, 't2m', 2)\n",
        "prac = spatial_lag_features(prac, prac.columns[3:], [1, 2, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "FcmsiOjot2Sm",
        "outputId": "c85b85c1-328e-499d-df8a-19ce4d7ae354"
      },
      "outputs": [],
      "source": [
        "prac"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
